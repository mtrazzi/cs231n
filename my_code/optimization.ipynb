{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing my own code (COPY-PASTING CODE NOT ALLOWED, DOC IS OK) for cs231n's classification notes\n",
    "\n",
    "this notebook is for: https://cs231n.github.io/optimization-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_cifar\n",
    "Xtr_raw, Ytr, Xte_raw, Yte = load_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X): return (X - X.mean()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding biases\n",
    "def add_bias(X): return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "Xtr, Xte = map(add_bias, map(preprocess, [Xtr_raw, Xte_raw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(np.unique(Ytr)).union(set(np.unique(Yte))))\n",
    "m, d = Xtr.shape\n",
    "ncl = len(labels)\n",
    "W = np.random.randn(ncl, d) * 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def L(self, X, y, W):\n",
    "        score = np.dot(W, X)\n",
    "        score_lab = score[y, np.arange(X.shape[1])]\n",
    "        loss =  np.sum(np.maximum(0, score - score_lab + 1)) - score_lab.shape[0]\n",
    "        return loss / X.shape[1]\n",
    "    def RandomSearch(self, X, y, ncl, n_iter=1000):\n",
    "        best_loss = np.inf\n",
    "        d = X.shape[0]\n",
    "        for i in range(n_iter):\n",
    "            W = np.random.randn(ncl, d) * 1e-4\n",
    "            loss = self.L(X, y, W)\n",
    "            if loss < best_loss:\n",
    "                Wbest = W\n",
    "                best_loss = loss\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                print(f\"after {i} iteration, best loss={best_loss}\")\n",
    "        return Wbest\n",
    "    def LocalSearch(self, X, y, ncl, n_iter=1000):\n",
    "        best_loss = np.inf\n",
    "        d = X.shape[0]\n",
    "        W = np.random.randn(ncl, d) * 1e-4\n",
    "        alpha = 1e-4\n",
    "        for i in range(n_iter):\n",
    "            delta = np.random.randn(ncl, d)\n",
    "            W_p = W + alpha * delta\n",
    "            loss = self.L(X, y, W_p)\n",
    "            if loss < best_loss:\n",
    "                W = W_p\n",
    "                best_loss = loss\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                print(f\"after {i} iteration, best loss={best_loss}\")\n",
    "        return Wbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "after 10 iteration, best loss=8.998284484420212\nafter 20 iteration, best loss=8.997094531477627\nafter 30 iteration, best loss=8.997094531477627\nafter 40 iteration, best loss=8.997094531477627\nafter 50 iteration, best loss=8.997094531477627\nafter 60 iteration, best loss=8.997094531477627\nafter 70 iteration, best loss=8.997094531477627\nafter 80 iteration, best loss=8.997094531477627\nafter 90 iteration, best loss=8.997094531477627\nafter 100 iteration, best loss=8.997094531477627\nafter 110 iteration, best loss=8.997094531477627\nafter 120 iteration, best loss=8.997094531477627\nafter 130 iteration, best loss=8.997094531477627\nafter 140 iteration, best loss=8.997094531477627\nafter 150 iteration, best loss=8.997094531477627\nafter 160 iteration, best loss=8.997094531477627\nafter 170 iteration, best loss=8.997094531477627\nafter 180 iteration, best loss=8.997094531477627\nafter 190 iteration, best loss=8.997094531477627\nafter 200 iteration, best loss=8.997094531477627\nafter 210 iteration, best loss=8.997094531477627\nafter 220 iteration, best loss=8.997094531477627\nafter 230 iteration, best loss=8.997094531477627\nafter 240 iteration, best loss=8.997094531477627\nafter 250 iteration, best loss=8.997094531477627\nafter 260 iteration, best loss=8.997094531477627\nafter 270 iteration, best loss=8.997094531477627\nafter 280 iteration, best loss=8.996584916123066\nafter 290 iteration, best loss=8.996584916123066\nafter 300 iteration, best loss=8.996584916123066\nafter 310 iteration, best loss=8.996584916123066\nafter 320 iteration, best loss=8.996584916123066\nafter 330 iteration, best loss=8.996584916123066\nafter 340 iteration, best loss=8.996584916123066\nafter 350 iteration, best loss=8.996584916123066\nafter 360 iteration, best loss=8.996584916123066\nafter 370 iteration, best loss=8.996584916123066\nafter 380 iteration, best loss=8.996584916123066\nafter 390 iteration, best loss=8.996584916123066\nafter 400 iteration, best loss=8.996584916123066\nafter 410 iteration, best loss=8.996584916123066\nafter 420 iteration, best loss=8.996584916123066\nafter 430 iteration, best loss=8.996584916123066\nafter 440 iteration, best loss=8.996584916123066\nafter 450 iteration, best loss=8.996584916123066\nafter 460 iteration, best loss=8.996584916123066\nafter 470 iteration, best loss=8.996584916123066\nafter 480 iteration, best loss=8.996584916123066\nafter 490 iteration, best loss=8.996584916123066\nafter 500 iteration, best loss=8.996584916123066\nafter 510 iteration, best loss=8.996584916123066\nafter 520 iteration, best loss=8.996584916123066\nafter 530 iteration, best loss=8.996584916123066\nafter 540 iteration, best loss=8.996584916123066\nafter 550 iteration, best loss=8.996584916123066\nafter 560 iteration, best loss=8.996584916123066\nafter 570 iteration, best loss=8.996584916123066\nafter 580 iteration, best loss=8.996584916123066\nafter 590 iteration, best loss=8.996584916123066\nafter 600 iteration, best loss=8.996584916123066\nafter 610 iteration, best loss=8.996584916123066\nafter 620 iteration, best loss=8.996584916123066\nafter 630 iteration, best loss=8.996584916123066\nafter 640 iteration, best loss=8.996584916123066\nafter 650 iteration, best loss=8.996584916123066\nafter 660 iteration, best loss=8.996584916123066\nafter 670 iteration, best loss=8.996584916123066\nafter 680 iteration, best loss=8.996584916123066\nafter 690 iteration, best loss=8.996584916123066\nafter 700 iteration, best loss=8.996584916123066\nafter 710 iteration, best loss=8.996584916123066\nafter 720 iteration, best loss=8.996584916123066\nafter 730 iteration, best loss=8.996584916123066\nafter 740 iteration, best loss=8.996584916123066\nafter 750 iteration, best loss=8.996584916123066\nafter 760 iteration, best loss=8.996584916123066\nafter 770 iteration, best loss=8.996584916123066\nafter 780 iteration, best loss=8.996584916123066\nafter 790 iteration, best loss=8.996584916123066\nafter 800 iteration, best loss=8.996584916123066\nafter 810 iteration, best loss=8.996584916123066\nafter 820 iteration, best loss=8.996584916123066\nafter 830 iteration, best loss=8.996584916123066\nafter 840 iteration, best loss=8.996584916123066\nafter 850 iteration, best loss=8.996584916123066\nafter 860 iteration, best loss=8.996584916123066\nafter 870 iteration, best loss=8.996584916123066\nafter 880 iteration, best loss=8.996584916123066\nafter 890 iteration, best loss=8.996584916123066\nafter 900 iteration, best loss=8.996584916123066\nafter 910 iteration, best loss=8.996584916123066\nafter 920 iteration, best loss=8.996584916123066\nafter 930 iteration, best loss=8.996584916123066\nafter 940 iteration, best loss=8.996584916123066\nafter 950 iteration, best loss=8.996584916123066\nafter 960 iteration, best loss=8.996584916123066\nafter 970 iteration, best loss=8.996584916123066\nafter 980 iteration, best loss=8.996584916123066\nafter 990 iteration, best loss=8.996584916123066\n"
    }
   ],
   "source": [
    "Wbest = clf.RandomSearch(Xtr.T, Ytr, ncl, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.1665"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "Yhat = np.argmax(np.dot(Wbest, Xte.T), axis=0)\n",
    "np.mean(Yte == Yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('cs231n': venv)",
   "language": "python",
   "name": "python37464bitcs231nvenvc0f208fee8534db4965080240acd3921"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}