{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing my own code (COPY-PASTING CODE NOT ALLOWED, DOC IS OK) for cs231n's classification notes\n",
    "\n",
    "this notebook is for: https://cs231n.github.io/optimization-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_cifar\n",
    "Xtr_raw, Ytr, Xte_raw, Yte = load_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X): return (X - X.mean()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding biases\n",
    "def add_bias(X): return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "Xtr, Xte = map(add_bias, map(preprocess, [Xtr_raw, Xte_raw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(np.unique(Ytr)).union(set(np.unique(Yte))))\n",
    "m, d = Xtr.shape\n",
    "ncl = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def L(self, X, y, W):\n",
    "        score = np.dot(W, X)\n",
    "        score_lab = score[y, np.arange(X.shape[1])]\n",
    "        loss =  np.sum(np.maximum(0, score - score_lab + 1)) - score_lab.shape[0]\n",
    "        return loss / X.shape[1]\n",
    "    def RandomSearch(self, X, y, ncl, n_iter=1000):\n",
    "        best_loss = np.inf\n",
    "        d = X.shape[0]\n",
    "        for i in range(n_iter):\n",
    "            W = np.random.randn(ncl, d) * 1e-4\n",
    "            loss = self.L(X, y, W)\n",
    "            if loss < best_loss:\n",
    "                Wbest = W\n",
    "                best_loss = loss\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                print(f\"after {i} iteration, best loss={best_loss}\")\n",
    "        return Wbest\n",
    "    def LocalSearch(self, X, y, ncl, n_iter=1000):\n",
    "        best_loss = np.inf\n",
    "        d = X.shape[0]\n",
    "        W = np.random.randn(ncl, d) * 1e-4\n",
    "        alpha = 1e-4\n",
    "        for i in range(n_iter):\n",
    "            delta = np.random.randn(ncl, d)\n",
    "            W_p = W + alpha * delta\n",
    "            loss = self.L(X, y, W_p)\n",
    "            if loss < best_loss:\n",
    "                W = W_p\n",
    "                best_loss = loss\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                print(f\"after {i} iteration, best loss={best_loss}\")\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(W):\n",
    "    Yhat = np.argmax(np.dot(W, Xte.T), axis=0)\n",
    "    print(f\"accuracy: {100 * np.mean(Yte == Yhat)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = Classifier()\n",
    "#Wbest = clf.RandomSearch(Xtr.T, Ytr, ncl, n_iter=1000)\n",
    "#test_accuracy(Wbest)\n",
    "#Wbest_local = clf.LocalSearch(Xtr.T, Ytr, ncl, n_iter=1000)\n",
    "#test_accuracy(Wbest_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, h=1e-5):\n",
    "    fx = f(x)\n",
    "    grad = np.zeros(x.shape)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp = x[idx]\n",
    "        x[idx] += h\n",
    "        fxh = f(x)\n",
    "        x[idx] = tmp\n",
    "        grad[idx] = (fxh - fx) / h\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(W, n_ex_per_loss=100):\n",
    "    clf = Classifier()\n",
    "    return clf.L(Xtr[:n_ex_per_loss].T, Ytr[:n_ex_per_loss], W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "gradient with 1 examples per loss took 9.956538s\n"
    }
   ],
   "source": [
    "import time\n",
    "n_ex_per_loss = 1\n",
    "start = time.time()\n",
    "W = np.random.randn(ncl, d) * 1e-4\n",
    "df = eval_numerical_gradient(cifar_loss, W)\n",
    "print(f\"gradient with {n_ex_per_loss} examples per loss took {time.time()-start:2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "gradient has been computed with 1 examples in the loss\nloss for alpha=1e-10 is 8.999640\nloss for alpha=1e-09 is 8.999639\nloss for alpha=1e-08 is 8.999636\nloss for alpha=1e-07 is 8.999606\nloss for alpha=1e-06 is 8.999306\nloss for alpha=1e-05 is 8.996306\nloss for alpha=0.0001 is 8.966308\nloss for alpha=0.001 is 8.666319\nloss for alpha=0.01 is 6.051703\nloss for alpha=0.1 is 5.931562\n"
    }
   ],
   "source": [
    "W = np.random.randn(ncl, d) * 1e-4\n",
    "loss_0 = cifar_loss(W)\n",
    "print(f\"gradient has been computed with {n_ex_per_loss} examples in the loss\")\n",
    "for i in list(np.arange(-10, 0)):\n",
    "    alpha = float(10) ** i\n",
    "    W_new = W - df * alpha\n",
    "    loss = cifar_loss(W_new)\n",
    "    print(f\"loss for alpha={alpha} is {loss:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_gradient(x, y, W):\n",
    "    d = x.shape[0]\n",
    "    ncl = W.shape[0]\n",
    "    score = np.dot(W, x)\n",
    "    grad = np.zeros_like(W)\n",
    "    sum_not_lab = np.sum(np.maximum(0, score - score[y] + 1)) - 1\n",
    "    for j in range(ncl):\n",
    "        if j != y:\n",
    "            grad[j] = ((score[j] - score[y] + 1) > 0) * x\n",
    "        else:\n",
    "            grad[j] = -sum_not_lab * x\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_gradient_all(X, y, W):\n",
    "    dw = np.zeros_like(W)\n",
    "    for j in range(X.shape[1]):\n",
    "        dw += svm_gradient(X[:,j], y[j], W)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10, 3073)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "out = svm_gradient(Xtr[0].T, Ytr[0], W)\n",
    "svm_gradient_all(Xtr[:10].T, Ytr[:10], W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vanilla_gradient_descent(W_init, n_steps=1):\n",
    "    W = W_init\n",
    "    alpha = 0.01\n",
    "    for step in range(n_steps):\n",
    "        print(f\"step #{step}\")\n",
    "        dW = svm_gradient_all(Xtr.T, Ytr, W)\n",
    "        W -= alpha * dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "step #0\naccuracy: 26.88%\n"
    }
   ],
   "source": [
    "\n",
    "vanilla_gradient_descent(W)\n",
    "test_accuracy(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    return data[random.sample(set(np.arange(data.shape[0])), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(W_init, n_steps=1, batch_size=256, alpha=1e-4):\n",
    "    W = W_init\n",
    "    for step in range(n_steps):\n",
    "        X, Y = map(lambda x: sample_batch(x, batch_size), [Xtr, Ytr])\n",
    "        dW = svm_gradient_all(X.T, Y, W)\n",
    "        W -= alpha * dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "batch #10\naccuracy: 10.639999999999999%\nloss=2.7159744433261036e+44\nbatch #20\naccuracy: 10.290000000000001%\nloss=6.827552204341103e+85\nbatch #30\naccuracy: 10.47%\nloss=1.734563596764155e+127\nbatch #40\naccuracy: 11.44%\nloss=4.0502986318315826e+168\nbatch #50\naccuracy: 11.33%\nloss=1.0308672365292907e+210\nbatch #60\naccuracy: 10.51%\nloss=2.6316739501843706e+251\nbatch #70\naccuracy: 10.5%\nloss=6.455395688706882e+292\nbatch #80\naccuracy: 10.0%\nloss=nan\nbatch #90\naccuracy: 10.0%\nloss=nan\n"
    }
   ],
   "source": [
    "# pls someone explain why accuracy doesn't go up here\n",
    "W = np.random.randn(ncl, d) * 1e-4\n",
    "alpha = 0.01\n",
    "for batch in range(100):\n",
    "    minibatch_gradient_descent(W, n_steps=1, batch_size=Xtr.shape[0], alpha=alpha)\n",
    "    if batch > 0 and batch % 10 == 0:\n",
    "        print(f\"batch #{batch}\")\n",
    "        test_accuracy(W)\n",
    "        print(f\"loss={cifar_loss(W, 100)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('cs231n': venv)",
   "language": "python",
   "name": "python37464bitcs231nvenvc0f208fee8534db4965080240acd3921"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}